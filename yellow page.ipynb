{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow Pages Data Scrapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Inputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_subject = \"new Car Dealers\"\n",
    "search_location = 'california'\n",
    "pages_to_scrape = 5\n",
    "site_name = \"Yellow Pages\"\n",
    "record_file = \"no\"\n",
    "url =\"https://www.yellowpages.com/search?search_terms=new+car+dealer&geo_location_terms=CA\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import httpx                         \n",
    "from fake_useragent import UserAgent\n",
    "import datetime\n",
    "import time\n",
    "from time import sleep\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = None\n",
    "'''the next page url, it's value is None for the first page scraped.'''\n",
    "\n",
    "page = 1\n",
    "'''the page number to be scraped.'''\n",
    "\n",
    "breaker = False \n",
    "'''json values in the controls.json file which terminates the scraping process if it's values is .'''\n",
    "\n",
    "results_scraped = 0\n",
    "'''counts the number of leads scraped.'''\n",
    "\n",
    "site = url\n",
    "'''the url of the page to be scraped, it's the original url for the first page.'''\n",
    "\n",
    "date_time = str( f'{datetime.datetime.now()}')[:f'{datetime.datetime.now()}'.index('.')].replace(':','.')\n",
    "'''the date and time in which the scraping process is initialized ,\n",
    "it will be add to the names of both the txt tracker file &\n",
    "the csv files which contains the scarped data'''\n",
    "\n",
    "primary_stage = True\n",
    "'''dictates the starting stage of the scraping process, if it's false then it will start from the secondary stage'''\n",
    "\n",
    "secondary_stage = True\n",
    "'''dictates the starting stage of the scraping process, if it's false then \n",
    "it will join the already saved PRIMARY & SECONDARY together.'''\n",
    "\n",
    "result_list =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking If There Was A File Which Contains The Record Of A Previous Uncompleted Scraping Process And Use It Complete Complete It, If It'S A New Scrape The Create A New Txt File To Record It'S Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if record_file.lower() != 'no':\n",
    "    txt_tracker = record_file\n",
    "else:\n",
    "    txt_tracker = f'tracker {search_subject} in {search_location} at {date_time}.txt'\n",
    "    first_creation = open('outputs/' + txt_tracker, \"w\")\n",
    "    first_creation.close()\n",
    "try:\n",
    "    with open('outputs/'+ txt_tracker , 'r') as file:\n",
    "        file = file.readlines()\n",
    "        '''an txt file which records the progress of the scraping process'''\n",
    "        \n",
    "        for line in file:\n",
    "            if 'https://' in line:           \n",
    "                site = line.strip(\"/n\")\n",
    "                \n",
    "            elif 'primary.csv' in line:\n",
    "                I = int(line[line.index(search_location):].split(' ')[1].strip(\"/n\"))\n",
    "                \n",
    "            elif 'PRIMARY' in line:\n",
    "                primary_stage = False \n",
    "                               \n",
    "            elif 'SECONDARY' in line:\n",
    "                secondary_stage = False\n",
    "  \n",
    "    save_index = I # they are equal as every page is save in it's own csv file\n",
    "    \n",
    "except:\n",
    "    save_index = 0\n",
    "    '''the number of the last saved primary or primary file\n",
    "    (incase of building upon old uncompleted scaping ), it's zero for \n",
    "    '''\n",
    "    I = 0\n",
    "    '''the number of the last page scraped.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Primary Scraper Code Cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 result page available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Primary Stage: 100%|\u001b[38;2;10;144;146m██████████████████████████████████████████████████████████\u001b[0m| 5/5 [00:20<00:00,  4.19s/page]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if primary_stage == True:\n",
    "\n",
    "    ua = UserAgent().random\n",
    "\n",
    "    # getting the request headers for the json file 'controls.json'\n",
    "    J = open('controls.json', 'r')\n",
    "    data = json.load(J)\n",
    "    header = data[\"headers\"]\n",
    "\n",
    "    # getting the no.of available results for a search subject   \n",
    "    session = httpx.Client()\n",
    "    session.headers = header\n",
    "    webpage = session.get(site)\n",
    "    path_soup = BeautifulSoup(webpage.content,\"html.parser\")\n",
    "    path = etree.HTML(str(path_soup))\n",
    "    soup = BeautifulSoup(webpage.text,\"html.parser\")\n",
    "\n",
    "\n",
    "    results_available_E = (path.xpath('//*[@id=\"main-content\"]//div[@class=\"pagination\"]/span'))[0].text\n",
    "    results_available = int(results_available_E.split('Showing ')[1].split(\" of\")[0].split(\"-\")[1])\n",
    "    print(results_available, \"result page available.\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        if pages_to_scrape > results_available:\n",
    "            pages_to_scrape = results_available  \n",
    "        pages_to_scrape =range(1,pages_to_scrape+1)\n",
    "    except:\n",
    "        pages_to_scrape = pages_to_scrape[I:]\n",
    "    \n",
    "    \n",
    "    for Page in tqdm(pages_to_scrape, desc='Primary Stage',unit= \"page\", ncols = 110, colour= '#0A9092'):\n",
    "        \n",
    "        # insuring the scrape of the fist page which used to get the available result\n",
    "        if next_page != None:\n",
    "            \n",
    "            while breaker == False: #if breaker tuned to true break the wile loop 'stop scraping'\n",
    "        \n",
    "                ua = UserAgent()\n",
    "                J = open('controls.json', 'r')\n",
    "                data = json.load(J)\n",
    "                headers = data[\"headers\"]\n",
    "                #nap: time in seconds which control the sleeping interval of request making incase of \n",
    "                # unsuccessful connection to the site servers.\n",
    "                nap = data[\"nap\"]\n",
    "                breaker = data[\"break\"] \n",
    "                \n",
    "                try: \n",
    "                    session = httpx.Client()\n",
    "                    session.headers = header\n",
    "                    webpage = session.get(site)\n",
    "                    path_soup = BeautifulSoup(webpage.content,\"html.parser\")\n",
    "                    path = etree.HTML(str(path_soup))\n",
    "                    soup = BeautifulSoup(webpage.text,\"html.parser\") \n",
    "                    break\n",
    "                \n",
    "                except:\n",
    "                    J = open('controls.json', 'r')\n",
    "                    data = json.load(J)\n",
    "                    headers = data[\"headers\"]\n",
    "                    nap = data[\"nap\"]\n",
    "                    breaker = data[\"break\"] \n",
    "                    sleep(nap)  \n",
    "                    \n",
    "                    \n",
    "        if breaker == True: #if breaker tuned to true break the for loop 'stop scraping'\n",
    "            break\n",
    "            \n",
    "        primary_info = soup.select('div[class=\"info-section info-primary\"]')# REMOVE THE S IN ELEMENTS AFTER DONE TESTING\n",
    "        secondary_info = soup.select('div[class=\"info-section info-secondary\"]') # searches for primary info container\n",
    "        \n",
    "        for P,S in zip(primary_info, secondary_info): # iterates through the results\n",
    "                    \n",
    "            business_E = P.find(class_='business-name')\n",
    "            \n",
    "            business = business_E.text # business name\n",
    "            \n",
    "            business_profile = \"https://www.yellowpages.com\"+business_E[\"href\"] # the business's profile \n",
    "            \n",
    "            try:\n",
    "                address_E = S.find(class_=\"adr\")#.text # gets the business location\n",
    "                address = address_E.find(class_= 'street-address').text+ \", \" + address_E.find(class_= 'locality').text\n",
    "            except:\n",
    "                address = \"Not Available\"\n",
    "            \n",
    "            try:\n",
    "                phone1 = S.select(\"div[class='phones phone primary']\")[0].text # gets the business phone number\n",
    "            except:\n",
    "                phone1 = \"Not Listed\"  \n",
    "                    \n",
    "            try:\n",
    "                site = P.select('a[class=\"track-visit-website\"]')[0]['href']# business website   \n",
    "            except:\n",
    "                site = \"Not Available\"      \n",
    "\n",
    "            try:\n",
    "                rating_E = P.select('div.ratings > a > div')[0]['class'][1:]\n",
    "                \n",
    "                if 'one' in rating_E[0]:\n",
    "                    rating = 1                \n",
    "                elif 'two' in rating_E[0]:\n",
    "                    rating = 2\n",
    "                elif 'three' in rating_E[0]:\n",
    "                    rating = 3\n",
    "                elif 'four' in rating_E[0]:\n",
    "                    rating = 4\n",
    "                else:\n",
    "                    rating = 5\n",
    "                    \n",
    "                if len(rating_E)==2:\n",
    "                    rating += 0.5                 \n",
    "            except:\n",
    "                rating = 0.0\n",
    "            \n",
    "            try:\n",
    "                review_count = int(P.select('div.ratings > a > span')[0].text[1:-1])\n",
    "            except:\n",
    "                review_count = 0\n",
    "            \n",
    "            result = {\"Business Name\": business,\n",
    "                    \"Profile\": business_profile,\n",
    "                    \"Phone Number\" :phone1,\n",
    "                    \"Address\": address,\n",
    "                    \"Website\": site, \n",
    "                    \"Rating\": rating, \n",
    "                    \"Review Count\": review_count\n",
    "                    }\n",
    "            \n",
    "            for key, value in result.items():\n",
    "                if value == '':\n",
    "                    result[key] = \"Not Listed\"\n",
    "                \n",
    "            result_list.append(result)\n",
    "        \n",
    "        # if len(result_list) >= 100: # saving the outputs to a csv file every 100 page\n",
    "        results_scraped += len(result_list)\n",
    "        save_index += 1\n",
    "        \n",
    "        m_df = pd.DataFrame(result_list)\n",
    "        m_df.to_csv('outputs/' + f\"{site_name}  {search_subject} in {search_location} {save_index} primary.csv\", index= False)\n",
    "        \n",
    "        result_list = []\n",
    "\n",
    "        with open('outputs/' + txt_tracker,'a') as file:\n",
    "            file.write(\n",
    "                '\\n' +\n",
    "                f\"{site_name}  {search_subject} in {search_location} {save_index} primary.csv\"+\n",
    "                '\\n'+\n",
    "                F'{site}'\n",
    "            )   \n",
    " \n",
    "        page +=1\n",
    "            \n",
    "        try:\n",
    "            previous_page = site\n",
    "            next_page = f'&page={page}'\n",
    "            site = url+ next_page\n",
    "            next = True\n",
    "            try:\n",
    "                site = site[:site.index('&User-Agent')]+site[site.index('&page'):]\n",
    "            except:\n",
    "                pass    \n",
    "        except:\n",
    "            print(\"scrapping is interrupted >>LAST PAGE ERROR<< !!\")\n",
    "            break\n",
    "\n",
    "        sleep(0.5)\n",
    "    file.close()\n",
    "else:\n",
    "    print('primary stage is already completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading The Individual Primary Files To Build One Data Frame And Save It As One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('outputs/' + txt_tracker,'r') as file:\n",
    "        contents = file.read()\n",
    "        P_csv_file = re.search('\\w+PRIMARY\\w+.csv',contents.replace(' ', '_'))[0].replace('_', ' ')\n",
    "        m_df = pd.read_csv('outputs/' + P_csv_file)\n",
    "        \n",
    "except:    \n",
    "    with open('outputs/' + txt_tracker,'r') as file:\n",
    "        for line in file: \n",
    "            if f' {search_location} 1 primary.csv' in line:\n",
    "                m_df0 = pd.read_csv('outputs/' + str(line).strip('\\n'))\n",
    "                \n",
    "            elif 'primary.csv' in line: \n",
    "                m_df = pd.concat([m_df0, pd.read_csv('outputs/' + str(line).strip('\\n'))], axis=0, ignore_index= True)\n",
    "                m_df0 = m_df\n",
    "  \n",
    "main_df = m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 151 entries, 0 to 150\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Business Name  151 non-null    object \n",
      " 1   Profile        151 non-null    object \n",
      " 2   Phone Number   151 non-null    object \n",
      " 3   Address        151 non-null    object \n",
      " 4   Website        151 non-null    object \n",
      " 5   Rating         151 non-null    float64\n",
      " 6   Review Count   151 non-null    int64  \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 8.4+ KB\n"
     ]
    }
   ],
   "source": [
    "main_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Business Name</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Address</th>\n",
       "      <th>Website</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Reliable Transportation</td>\n",
       "      <td>https://www.yellowpages.com/vernon-ca/mip/reli...</td>\n",
       "      <td>(323) 813-0120</td>\n",
       "      <td>2043 Ross St, Vernon, CA 90058</td>\n",
       "      <td>http://www.reliable-transportation.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Khm Inc</td>\n",
       "      <td>https://www.yellowpages.com/los-angeles-ca/mip...</td>\n",
       "      <td>(213) 388-2020</td>\n",
       "      <td>3071 W Olympic Blvd, Los Angeles, CA 90006</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Electric Car Sales &amp; Service</td>\n",
       "      <td>https://www.yellowpages.com/long-beach-ca/mip/...</td>\n",
       "      <td>(562) 436-6241</td>\n",
       "      <td>3850 Cherry Ave, Long Beach, CA 90807</td>\n",
       "      <td>http://electriccarsalesandservice.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Professional Leasing</td>\n",
       "      <td>https://www.yellowpages.com/redwood-city-ca/mi...</td>\n",
       "      <td>(650) 361-8838</td>\n",
       "      <td>260 Main St, Redwood City, CA 94063</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>De Anza Porsche</td>\n",
       "      <td>https://www.yellowpages.com/riverside-ca/mip/d...</td>\n",
       "      <td>(909) 824-2150</td>\n",
       "      <td>3150 Adams St, Riverside, CA 92504</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Business Name  \\\n",
       "146       Reliable Transportation   \n",
       "147                       Khm Inc   \n",
       "148  Electric Car Sales & Service   \n",
       "149          Professional Leasing   \n",
       "150               De Anza Porsche   \n",
       "\n",
       "                                               Profile    Phone Number  \\\n",
       "146  https://www.yellowpages.com/vernon-ca/mip/reli...  (323) 813-0120   \n",
       "147  https://www.yellowpages.com/los-angeles-ca/mip...  (213) 388-2020   \n",
       "148  https://www.yellowpages.com/long-beach-ca/mip/...  (562) 436-6241   \n",
       "149  https://www.yellowpages.com/redwood-city-ca/mi...  (650) 361-8838   \n",
       "150  https://www.yellowpages.com/riverside-ca/mip/d...  (909) 824-2150   \n",
       "\n",
       "                                        Address  \\\n",
       "146              2043 Ross St, Vernon, CA 90058   \n",
       "147  3071 W Olympic Blvd, Los Angeles, CA 90006   \n",
       "148       3850 Cherry Ave, Long Beach, CA 90807   \n",
       "149         260 Main St, Redwood City, CA 94063   \n",
       "150          3150 Adams St, Riverside, CA 92504   \n",
       "\n",
       "                                    Website  Rating  Review Count  \n",
       "146  http://www.reliable-transportation.com     0.0             0  \n",
       "147                           Not Available     0.0             0  \n",
       "148   http://electriccarsalesandservice.com     0.0             0  \n",
       "149                           Not Available     0.0             0  \n",
       "150                           Not Available     0.0             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_csv('outputs/' + f\"{site_name} PRIMARY {search_subject} in {search_location}.csv\", index= False)\n",
    "\n",
    "with open('outputs/' + txt_tracker,'a') as file:         \n",
    "    file.write('\\n')\n",
    "    file.write (f\"{site_name} PRIMARY {search_subject} in {search_location}.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the individual primary files after saving the concatenated PRIMARY file\n",
    "files = os.listdir('outputs/')\n",
    "for file in files:\n",
    "    if 'primary' in file:\n",
    "        os.remove('outputs/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 151 entries, 0 to 150\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Business Name  151 non-null    object \n",
      " 1   Profile        151 non-null    object \n",
      " 2   Phone Number   151 non-null    object \n",
      " 3   Address        151 non-null    object \n",
      " 4   Website        151 non-null    object \n",
      " 5   Rating         151 non-null    float64\n",
      " 6   Review Count   151 non-null    int64  \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 8.4+ KB\n"
     ]
    }
   ],
   "source": [
    "main_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"json values in the controls.json file which terminates the scraping process if it's values is .\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separating the main list and the scrapped page variable to make prevent data loss when relaunching the scraper in \n",
    "# case of connection interruption\n",
    "contacts = []\n",
    "\n",
    "profile_no = 0\n",
    "'''counts the number of leads scraped.'''\n",
    "\n",
    "breaker = False\n",
    "'''json values in the controls.json file which terminates the scraping process if it's values is .'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking The Txt File If There Are Secondary Files Already Scraped To Continue Upon It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    file = open('outputs/' + txt_tracker, 'r').readlines()\n",
    "    for line in file:\n",
    "        if 'secondary.csv' not in line and not 'https://www' in line:\n",
    "            I = int(line.strip(\"/n\"))\n",
    "            \"\"\"the index of the last scraped profile url in it's secondary df\"\"\"\n",
    "            \n",
    "        elif 'secondary.csv'  in line:\n",
    "            save_index = int(line[line.index(search_location):].split(' ')[1].strip(\"/n\"))\n",
    "            \"\"\"the number of the last saved secondary csv file\"\"\"\n",
    "except:\n",
    "    I = 0 \n",
    "    save_index = 0\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Secondary Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Secondary Stage: 100%|\u001b[38;2;10;144;146m█████████████████████████████████████████████████\u001b[0m| 151/151 [18:22<00:00,  7.30s/profile]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if secondary_stage == True:\n",
    "    \n",
    "    for profile in tqdm(list(main_df[\"Profile\"][I:]), desc='Secondary Stage',unit= \"profile\", ncols = 110, colour= '#0A9092'):\n",
    "    \n",
    "        while True:\n",
    "            ua = UserAgent()\n",
    "            J = open('controls.json', 'r')\n",
    "            data = json.load(J)\n",
    "            headers = data[\"headers\"]\n",
    "            nap = data[\"nap\"]\n",
    "            break_ = data[\"break\"]\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                session = httpx.Client()\n",
    "                session.headers = headers\n",
    "                webpage = session.get(profile)\n",
    "                path_soup = BeautifulSoup(webpage.content,\"html.parser\")\n",
    "                path = etree.HTML(str(path_soup))\n",
    "                soup = BeautifulSoup(webpage.text,\"html.parser\")\n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                ua = UserAgent().random\n",
    "                sleep(nap)\n",
    "                \n",
    "        if breaker == True:\n",
    "            break\n",
    "        \n",
    "    # catagories\n",
    "        try:\n",
    "            catagories_E = soup.select('dd.categories > div.categories > a')\n",
    "            catagories = \"\"\n",
    "            for cat in catagories_E:\n",
    "                catagories+= cat.text + \", \"\n",
    "            catagories = catagories.strip(', ')\n",
    "        except:\n",
    "            catagories= \"Not Listed\"\n",
    "        \n",
    "        \n",
    "    # neighborhoods\n",
    "        try:\n",
    "            neighborhoods_E = soup.select('dd.neighborhoods > span > a')\n",
    "            neighborhoods = \"\"\n",
    "            for N in neighborhoods_E:\n",
    "                neighborhoods+= N.text + \", \"\n",
    "            neighborhoods = neighborhoods.strip(', ')\n",
    "        except:\n",
    "            neighborhoods = \"Not Listed\"\n",
    "        \n",
    "        \n",
    "    #extra phones\n",
    "        try:\n",
    "            extra_phones_E = soup.select('dd.extra-phones > P')\n",
    "            extra_phones = \"\"\n",
    "            for P in extra_phones_E:\n",
    "                extra_phones += P.text.replace('\\xa0','') + \"\\n\"\n",
    "            extra_phones = extra_phones.strip('\\n')\n",
    "        except:\n",
    "            extra_phones= \"Not Listed\"\n",
    "        \n",
    "    # emails\n",
    "        try:\n",
    "            email_E = soup.select('a[class=\"email-business\"]')\n",
    "            email = \"\"\n",
    "            for M in email_E:\n",
    "                email += 'http://www.'+ (M['href'].split(\":\")[1]) + \"\\n\"\n",
    "            email = email.strip('\\n')\n",
    "        except:\n",
    "            email= \"Not Listed\"\n",
    "        \n",
    "        \n",
    "    # other links\n",
    "        try:\n",
    "            other_links_E = soup.select('a[class=\"other-links\"]')\n",
    "            other_links = \"\"\n",
    "            for L in other_links_E:\n",
    "                other_links += L['href'] + \"\\n\"\n",
    "            other_links = other_links.strip('\\n')\n",
    "        except:\n",
    "            other_links= \"Not Listed\"\n",
    "        \n",
    "\n",
    "    # social links\n",
    "        try:\n",
    "            social_links_E = soup.select('dd.social-links')[0].find_all('a')\n",
    "            social_links = \"\"\n",
    "            for S in social_links_E:\n",
    "                social_links += S['href'] + \"\\n\"\n",
    "            social_links = social_links.strip('\\n')\n",
    "        except:\n",
    "            social_links= \"Not Listed\"\n",
    "        \n",
    "        B_contacts = {\n",
    "                    \"Catagories\": catagories,\n",
    "                    \"Neighborhoods\": neighborhoods,\n",
    "                    \"Extra Phones\": extra_phones,\n",
    "                    \"e-mail\": email, \n",
    "                    \"Other Links\": other_links,\n",
    "                    \"Social Media\": social_links\n",
    "                    }\n",
    "        \n",
    "        for key, value in B_contacts.items():\n",
    "            if value == '':\n",
    "                B_contacts[key] = \"Not Listed\"\n",
    "                        \n",
    "        contacts.append(B_contacts)\n",
    "        profile_no += 1\n",
    "        \n",
    "        if len(contacts) == 30: # saving the outputs to a csv file every 30 page\n",
    "            results_scraped += len(contacts)\n",
    "            save_index += 1\n",
    "            \n",
    "            s_df = pd.DataFrame(contacts)\n",
    "            s_df.to_csv('outputs/' + f\"{site_name} {search_subject} in {search_location} {save_index} secondary.csv\", index= False)  \n",
    "            \n",
    "            contacts = []\n",
    "            \n",
    "            with open('outputs/' + txt_tracker,'a') as file:\n",
    "                file.write(\n",
    "                    '\\n' +\n",
    "                    f\"{site_name} {search_subject} in {search_location} {save_index} secondary.csv\"+\n",
    "                    '\\n'+\n",
    "                    F'{profile_no}'\n",
    "                )  \n",
    "            \n",
    "            \n",
    "        previous_page = profile\n",
    "        sleep(0.5) \n",
    "        \n",
    "    if len(contacts) > 0: # saving the remaining output if the scraping is ended and the no of pages scraped is less than 30\n",
    "        s_df0 = pd.DataFrame(contacts)\n",
    "        s_df0.to_csv('outputs/' + f\"{site_name} {search_subject} in {search_location} 0 secondary.csv\", index= False)\n",
    "        file = open('outputs/' + txt_tracker,'a')\n",
    "        file.write('\\n')\n",
    "        file.write (f\"{site_name} {search_subject} in {search_location} 0 secondary.csv\") \n",
    "        file.write('\\n')   \n",
    "        file.close()\n",
    "else:\n",
    "    print(\"secondary file is already been scraped and saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating The Individually Saved Secondary csv Files Into One File. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "try:#checking if there was SECONDARY file already saved\n",
    "    with open('outputs/' + txt_tracker,'r') as file:\n",
    "        contents = file.read()\n",
    "        P_csv_file = re.search('\\w+SECONDARY\\w+.csv',contents.replace(' ', '_'))[0].replace('_', ' ')\n",
    "        m_df = pd.read_csv('outputs/' + P_csv_file) \n",
    "        \n",
    "except:\n",
    "    \n",
    "    with open('outputs/' + txt_tracker,'r') as file:\n",
    "        lines = file.readlines()  \n",
    "        for line in lines[len(lines)::-1]: \n",
    "            if  f'{save_index} secondary.csv' in line:\n",
    "                s_df0 = pd.read_csv('outputs/' + str(line).strip('\\n'))\n",
    "            \n",
    "            elif 'secondary.csv' in line:\n",
    "                s_df = pd.concat([s_df0, pd.read_csv('outputs/' + str(line).strip('\\n'))], axis=0, ignore_index= True)\n",
    "                s_df0 = s_df\n",
    "\n",
    "            elif 'primary.csv' in line: \n",
    "                break\n",
    "\n",
    "secondary_df = s_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Catagories     150 non-null    object\n",
      " 1   Neighborhoods  150 non-null    object\n",
      " 2   Extra Phones   150 non-null    object\n",
      " 3   e-mail         150 non-null    object\n",
      " 4   Other Links    150 non-null    object\n",
      " 5   Social Media   150 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 7.2+ KB\n"
     ]
    }
   ],
   "source": [
    "secondary_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catagories</th>\n",
       "      <th>Neighborhoods</th>\n",
       "      <th>Extra Phones</th>\n",
       "      <th>e-mail</th>\n",
       "      <th>Other Links</th>\n",
       "      <th>Social Media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>New Car Dealers, Auto Repair &amp; Service, Automo...</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>http://www.teamnissan@teamnissandirect.com</td>\n",
       "      <td>http://www.teamnissandirect.com</td>\n",
       "      <td>https://facebook.com/teamnissanoxnard\\nteamnis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>New Car Dealers, Auto Oil &amp; Lube, Auto Repair ...</td>\n",
       "      <td>Downtown Campbell, Central Campbell</td>\n",
       "      <td>TollFree:(800) 367-3872</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>http://local.firestonecompleteautocare.com/cal...</td>\n",
       "      <td>https://www.facebook.com/FirestoneCompleteAuto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>New Car Dealers, Auto Body Parts, Auto Repair ...</td>\n",
       "      <td>Central Alameda, South LA</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>http://www.ajp2207@gmail.com</td>\n",
       "      <td>https://www.aplusfordandchevy.com</td>\n",
       "      <td>Not Listed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>New Car Dealers, Auto Repair &amp; Service, Automo...</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>http://www.slowery@corningford.com\\nhttp://www...</td>\n",
       "      <td>http://www.redbluffdodgechryslerjeepram.com</td>\n",
       "      <td>Not Listed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>New Car Dealers, New Truck Dealers</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>http://www.sales@corningford.com\\nhttp://www.s...</td>\n",
       "      <td>http://www.corningford.com</td>\n",
       "      <td>https://www.facebook.com/CorningFord/timeline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Catagories  \\\n",
       "145  New Car Dealers, Auto Repair & Service, Automo...   \n",
       "146  New Car Dealers, Auto Oil & Lube, Auto Repair ...   \n",
       "147  New Car Dealers, Auto Body Parts, Auto Repair ...   \n",
       "148  New Car Dealers, Auto Repair & Service, Automo...   \n",
       "149                 New Car Dealers, New Truck Dealers   \n",
       "\n",
       "                           Neighborhoods             Extra Phones  \\\n",
       "145                           Not Listed               Not Listed   \n",
       "146  Downtown Campbell, Central Campbell  TollFree:(800) 367-3872   \n",
       "147            Central Alameda, South LA               Not Listed   \n",
       "148                           Not Listed               Not Listed   \n",
       "149                           Not Listed               Not Listed   \n",
       "\n",
       "                                                e-mail  \\\n",
       "145         http://www.teamnissan@teamnissandirect.com   \n",
       "146                                         Not Listed   \n",
       "147                       http://www.ajp2207@gmail.com   \n",
       "148  http://www.slowery@corningford.com\\nhttp://www...   \n",
       "149  http://www.sales@corningford.com\\nhttp://www.s...   \n",
       "\n",
       "                                           Other Links  \\\n",
       "145                    http://www.teamnissandirect.com   \n",
       "146  http://local.firestonecompleteautocare.com/cal...   \n",
       "147                  https://www.aplusfordandchevy.com   \n",
       "148        http://www.redbluffdodgechryslerjeepram.com   \n",
       "149                         http://www.corningford.com   \n",
       "\n",
       "                                          Social Media  \n",
       "145  https://facebook.com/teamnissanoxnard\\nteamnis...  \n",
       "146  https://www.facebook.com/FirestoneCompleteAuto...  \n",
       "147                                         Not Listed  \n",
       "148                                         Not Listed  \n",
       "149      https://www.facebook.com/CorningFord/timeline  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondary_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_df.to_csv('outputs/' + f\"{site_name} SECONDARY {search_subject} in {search_location}.csv\", index= False)\n",
    "\n",
    "with open('outputs/' + txt_tracker,'a') as file:         \n",
    "    file.write('\\n')\n",
    "    file.write (f\"{site_name} SECONDARY {search_subject} in {search_location}.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting the secondary files after saving the concatenated SECONDARY file\n",
    "files = os.listdir('outputs/')\n",
    "for file in files:\n",
    "    if 'secondary' in file:\n",
    "        os.remove('outputs/' + file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining The secondary_df With the main_df To Get The Final Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = main_df.join(secondary_df)\n",
    "df_combined.index += 1  # make the index start from one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 151 entries, 1 to 151\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Business Name  151 non-null    object \n",
      " 1   Profile        151 non-null    object \n",
      " 2   Phone Number   151 non-null    object \n",
      " 3   Address        151 non-null    object \n",
      " 4   Website        151 non-null    object \n",
      " 5   Rating         151 non-null    float64\n",
      " 6   Review Count   151 non-null    int64  \n",
      " 7   Catagories     150 non-null    object \n",
      " 8   Neighborhoods  150 non-null    object \n",
      " 9   Extra Phones   150 non-null    object \n",
      " 10  e-mail         150 non-null    object \n",
      " 11  Other Links    150 non-null    object \n",
      " 12  Social Media   150 non-null    object \n",
      "dtypes: float64(1), int64(1), object(11)\n",
      "memory usage: 15.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Business Name</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Address</th>\n",
       "      <th>Website</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Count</th>\n",
       "      <th>Catagories</th>\n",
       "      <th>Neighborhoods</th>\n",
       "      <th>Extra Phones</th>\n",
       "      <th>e-mail</th>\n",
       "      <th>Other Links</th>\n",
       "      <th>Social Media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LemonBuyBack.com-Law Offices of Jon Jacobs</td>\n",
       "      <td>https://www.yellowpages.com/nationwide/mip/lem...</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>New Car Dealers</td>\n",
       "      <td>Chesterfield Square, Congress Central, South L...</td>\n",
       "      <td>Phone:(323) 293-6850\\nPhone:(323) 293-0977\\nFa...</td>\n",
       "      <td>http://www.info@lincolnucc.org</td>\n",
       "      <td>http://www.lincolnucc.org\\nhttps://lincolnucc....</td>\n",
       "      <td>Not Listed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LemonBuyBack.com-Law Offices of Jon Jacobs</td>\n",
       "      <td>https://www.yellowpages.com/nationwide/mip/lem...</td>\n",
       "      <td>(916) 626-4164</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>http://lemonbuyback.com</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Automobile Body Repairing &amp; Painting, Auto Rep...</td>\n",
       "      <td>North Hollywood North East, South Valley</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>http://www.G.David@autobahncenter.com</td>\n",
       "      <td>http://www.autobahncenter.com</td>\n",
       "      <td>Not Listed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El Cajon Ford</td>\n",
       "      <td>https://www.yellowpages.com/el-cajon-ca/mip/el...</td>\n",
       "      <td>(619) 333-8825</td>\n",
       "      <td>1595 E Main St, El Cajon, CA 92021</td>\n",
       "      <td>http://www.elcajonford.com</td>\n",
       "      <td>5.0</td>\n",
       "      <td>43</td>\n",
       "      <td>Automobile Parts &amp; Supplies, Auto Repair &amp; Ser...</td>\n",
       "      <td>Hollywood Hills West, Central LA</td>\n",
       "      <td>Phone:(310) 271-3476\\nPhone:(310) 274-3476\\nPh...</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>http://www.universalcitynissan.com</td>\n",
       "      <td>Not Listed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jaguar Coventry Cars of San Diego</td>\n",
       "      <td>https://www.yellowpages.com/san-diego-ca/mip/j...</td>\n",
       "      <td>(619) 297-9393</td>\n",
       "      <td>5097 Santa Fe St, San Diego, CA 92109</td>\n",
       "      <td>http://www.coventryjaguar.com</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3</td>\n",
       "      <td>Automobile &amp; Truck Brokers, Automobile Parts &amp;...</td>\n",
       "      <td>Rosemead</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Allied Auto Works</td>\n",
       "      <td>https://www.yellowpages.com/los-altos-ca/mip/a...</td>\n",
       "      <td>(650) 209-6897</td>\n",
       "      <td>2073 Grant Rd, Los Altos, CA 94024</td>\n",
       "      <td>http://www.alliedautoworks.com</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>New Car Dealers</td>\n",
       "      <td>Tustin</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "      <td>Not Listed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Business Name  \\\n",
       "1  LemonBuyBack.com-Law Offices of Jon Jacobs   \n",
       "2  LemonBuyBack.com-Law Offices of Jon Jacobs   \n",
       "3                               El Cajon Ford   \n",
       "4           Jaguar Coventry Cars of San Diego   \n",
       "5                           Allied Auto Works   \n",
       "\n",
       "                                             Profile    Phone Number  \\\n",
       "1  https://www.yellowpages.com/nationwide/mip/lem...      Not Listed   \n",
       "2  https://www.yellowpages.com/nationwide/mip/lem...  (916) 626-4164   \n",
       "3  https://www.yellowpages.com/el-cajon-ca/mip/el...  (619) 333-8825   \n",
       "4  https://www.yellowpages.com/san-diego-ca/mip/j...  (619) 297-9393   \n",
       "5  https://www.yellowpages.com/los-altos-ca/mip/a...  (650) 209-6897   \n",
       "\n",
       "                                 Address                         Website  \\\n",
       "1                          Not Available                   Not Available   \n",
       "2                          Not Available         http://lemonbuyback.com   \n",
       "3     1595 E Main St, El Cajon, CA 92021      http://www.elcajonford.com   \n",
       "4  5097 Santa Fe St, San Diego, CA 92109   http://www.coventryjaguar.com   \n",
       "5     2073 Grant Rd, Los Altos, CA 94024  http://www.alliedautoworks.com   \n",
       "\n",
       "   Rating  Review Count                                         Catagories  \\\n",
       "1     0.0             0                                    New Car Dealers   \n",
       "2     5.0            10  Automobile Body Repairing & Painting, Auto Rep...   \n",
       "3     5.0            43  Automobile Parts & Supplies, Auto Repair & Ser...   \n",
       "4     4.5             3  Automobile & Truck Brokers, Automobile Parts &...   \n",
       "5     0.0             0                                    New Car Dealers   \n",
       "\n",
       "                                       Neighborhoods  \\\n",
       "1  Chesterfield Square, Congress Central, South L...   \n",
       "2           North Hollywood North East, South Valley   \n",
       "3                   Hollywood Hills West, Central LA   \n",
       "4                                           Rosemead   \n",
       "5                                             Tustin   \n",
       "\n",
       "                                        Extra Phones  \\\n",
       "1  Phone:(323) 293-6850\\nPhone:(323) 293-0977\\nFa...   \n",
       "2                                         Not Listed   \n",
       "3  Phone:(310) 271-3476\\nPhone:(310) 274-3476\\nPh...   \n",
       "4                                         Not Listed   \n",
       "5                                         Not Listed   \n",
       "\n",
       "                                  e-mail  \\\n",
       "1         http://www.info@lincolnucc.org   \n",
       "2  http://www.G.David@autobahncenter.com   \n",
       "3                             Not Listed   \n",
       "4                             Not Listed   \n",
       "5                             Not Listed   \n",
       "\n",
       "                                         Other Links Social Media  \n",
       "1  http://www.lincolnucc.org\\nhttps://lincolnucc....   Not Listed  \n",
       "2                      http://www.autobahncenter.com   Not Listed  \n",
       "3                 http://www.universalcitynissan.com   Not Listed  \n",
       "4                                         Not Listed   Not Listed  \n",
       "5                                         Not Listed   Not Listed  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Saving The Completed DataFrame df_combined & Deleting The Main File & The Secondary File As They No Longer Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.to_csv('outputs/' + f\"{site_name} {search_subject} in {search_location}.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting both the PRIMARY and SECONDARY files after saving the joined end result\n",
    "files = os.listdir('outputs/')\n",
    "for file in files:\n",
    "    if 'PRIMARY' in file or 'SECONDARY' in file or file == txt_tracker:\n",
    "        os.remove('outputs/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8823af32175d41bfca297386eea8c1e23370595e130acdcb57ee9569b0d429e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
